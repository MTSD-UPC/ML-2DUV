nohup: ignoring input
2022-04-28 10:28:02.286658: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-04-28 10:28:03.920363: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-04-28 10:28:03.921422: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-04-28 10:28:04.083227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2022-04-28 10:28:04.084533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2022-04-28 10:28:04.084599: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-04-28 10:28:04.089320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-04-28 10:28:04.089454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-04-28 10:28:04.090740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-04-28 10:28:04.091053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-04-28 10:28:04.094361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-04-28 10:28:04.095104: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-04-28 10:28:04.095257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-04-28 10:28:04.097756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2022-04-28 10:28:04.098446: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-28 10:28:04.102738: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-04-28 10:28:04.398956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2022-04-28 10:28:04.399799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2022-04-28 10:28:04.399850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-04-28 10:28:04.399900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-04-28 10:28:04.399922: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-04-28 10:28:04.399945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-04-28 10:28:04.399966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-04-28 10:28:04.399986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-04-28 10:28:04.400027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-04-28 10:28:04.400051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-04-28 10:28:04.402948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2022-04-28 10:28:04.403060: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-04-28 10:28:05.465342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-04-28 10:28:05.465407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 
2022-04-28 10:28:05.465418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N 
2022-04-28 10:28:05.465424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N 
2022-04-28 10:28:05.468404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10072 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5)
2022-04-28 10:28:05.470163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10072 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]
['a-helix' 'b-sheet' 'other-SS']
[0 1 2]
Search space summary
Default search space size: 4
filters (Choice)
{'default': 256, 'conditions': [], 'values': [32, 64, 128, 256, 512], 'ordered': True}
max_pooling_num (Int)
{'default': 12, 'conditions': [], 'min_value': 2, 'max_value': 20, 'step': 2, 'sampling': None}
leraning_rate (Choice)
{'default': 0.001, 'conditions': [], 'values': [0.01, 0.001, 0.002, 0.008, 0.004, 0.0001, 0.0004, 0.0008], 'ordered': True}
drop_out_rate (Float)
{'default': 0.2, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': None}
None

Search: Running Trial #1

Hyperparameter    |Value             |Best Value So Far 
filters           |256               |512               
max_pooling_num   |12                |10                
leraning_rate     |0.01              |0.0004            
drop_out_rate     |0.2               |0.3               

2022-04-28 10:28:06.379365: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_643"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-04-28 10:28:06.389526: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2022-04-28 10:28:06.390158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099990000 Hz
Epoch 1/20
2022-04-28 10:28:10.278490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-04-28 10:28:10.837180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-04-28 10:28:11.392796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-04-28 10:28:16.324575: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_2817"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 12s - loss: 1.6575 - acc: 0.3843 - val_loss: 0.9111 - val_acc: 0.6042
Epoch 2/20
14/14 - 1s - loss: 0.9135 - acc: 0.5689 - val_loss: 0.8332 - val_acc: 0.6875
Epoch 3/20
14/14 - 1s - loss: 0.8598 - acc: 0.6262 - val_loss: 0.7946 - val_acc: 0.6481
Epoch 4/20
14/14 - 1s - loss: 0.7921 - acc: 0.6481 - val_loss: 0.6985 - val_acc: 0.7106
Epoch 5/20
14/14 - 1s - loss: 0.7853 - acc: 0.6580 - val_loss: 0.6909 - val_acc: 0.7060
Epoch 6/20
14/14 - 1s - loss: 0.7599 - acc: 0.6759 - val_loss: 0.6393 - val_acc: 0.7315
Epoch 7/20
14/14 - 1s - loss: 0.6870 - acc: 0.7037 - val_loss: 0.6362 - val_acc: 0.7523
Epoch 8/20
14/14 - 1s - loss: 0.6442 - acc: 0.7292 - val_loss: 0.5840 - val_acc: 0.7639
Epoch 9/20
14/14 - 1s - loss: 0.6261 - acc: 0.7500 - val_loss: 0.5721 - val_acc: 0.7685
Epoch 10/20
14/14 - 1s - loss: 0.6091 - acc: 0.7546 - val_loss: 0.5534 - val_acc: 0.7870
Epoch 11/20
14/14 - 1s - loss: 0.5642 - acc: 0.7824 - val_loss: 0.5949 - val_acc: 0.7593
Epoch 12/20
14/14 - 1s - loss: 0.5632 - acc: 0.7720 - val_loss: 0.5505 - val_acc: 0.7731
Epoch 13/20
14/14 - 1s - loss: 0.5406 - acc: 0.7836 - val_loss: 0.5112 - val_acc: 0.8032
Epoch 14/20
14/14 - 1s - loss: 0.5291 - acc: 0.7905 - val_loss: 0.5698 - val_acc: 0.7824
Epoch 15/20
14/14 - 1s - loss: 0.5816 - acc: 0.7691 - val_loss: 0.5372 - val_acc: 0.7917
Epoch 16/20
14/14 - 1s - loss: 0.5373 - acc: 0.7940 - val_loss: 0.5350 - val_acc: 0.8079
Epoch 17/20
14/14 - 1s - loss: 0.5400 - acc: 0.7870 - val_loss: 0.5181 - val_acc: 0.8079
Epoch 18/20
14/14 - 1s - loss: 0.5344 - acc: 0.7934 - val_loss: 0.4751 - val_acc: 0.8194
Epoch 19/20
14/14 - 1s - loss: 0.5192 - acc: 0.7934 - val_loss: 0.5326 - val_acc: 0.7963
Epoch 20/20
14/14 - 1s - loss: 0.5165 - acc: 0.8009 - val_loss: 0.5105 - val_acc: 0.8032

Trial 1 Complete [00h 00m 23s]
val_acc: 0.8194444179534912

Best val_acc So Far: 0.8194444179534912
Total elapsed time: 00h 00m 23s

Search: Running Trial #2

Hyperparameter    |Value             |Best Value So Far 
filters           |128               |256               
max_pooling_num   |14                |12                
leraning_rate     |0.002             |0.01              
drop_out_rate     |0.2               |0.2               

2022-04-28 10:28:30.401209: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_9203"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:28:34.542875: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_11377"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0038 - acc: 0.4803 - val_loss: 0.8598 - val_acc: 0.6296
Epoch 2/20
14/14 - 0s - loss: 0.8516 - acc: 0.6262 - val_loss: 0.7556 - val_acc: 0.6875
Epoch 3/20
14/14 - 0s - loss: 0.7834 - acc: 0.6626 - val_loss: 0.7368 - val_acc: 0.6620
Epoch 4/20
14/14 - 0s - loss: 0.7090 - acc: 0.6968 - val_loss: 0.6396 - val_acc: 0.7407
Epoch 5/20
14/14 - 0s - loss: 0.6480 - acc: 0.7355 - val_loss: 0.5913 - val_acc: 0.7569
Epoch 6/20
14/14 - 0s - loss: 0.6035 - acc: 0.7598 - val_loss: 0.5678 - val_acc: 0.7755
Epoch 7/20
14/14 - 0s - loss: 0.6018 - acc: 0.7650 - val_loss: 0.5581 - val_acc: 0.7917
Epoch 8/20
14/14 - 0s - loss: 0.5819 - acc: 0.7679 - val_loss: 0.5429 - val_acc: 0.7940
Epoch 9/20
14/14 - 0s - loss: 0.5555 - acc: 0.7853 - val_loss: 0.5349 - val_acc: 0.7963
Epoch 10/20
14/14 - 0s - loss: 0.5632 - acc: 0.7824 - val_loss: 0.5471 - val_acc: 0.7847
Epoch 11/20
14/14 - 0s - loss: 0.5481 - acc: 0.7888 - val_loss: 0.5248 - val_acc: 0.7986
Epoch 12/20
14/14 - 0s - loss: 0.5309 - acc: 0.8015 - val_loss: 0.5307 - val_acc: 0.7894
Epoch 13/20
14/14 - 0s - loss: 0.5109 - acc: 0.8027 - val_loss: 0.5234 - val_acc: 0.7894
Epoch 14/20
14/14 - 0s - loss: 0.5217 - acc: 0.7940 - val_loss: 0.5334 - val_acc: 0.7917
Epoch 15/20
14/14 - 0s - loss: 0.5059 - acc: 0.8079 - val_loss: 0.4974 - val_acc: 0.8171
Epoch 16/20
14/14 - 0s - loss: 0.5059 - acc: 0.8050 - val_loss: 0.5126 - val_acc: 0.8032
Epoch 17/20
14/14 - 0s - loss: 0.5005 - acc: 0.8009 - val_loss: 0.4921 - val_acc: 0.8148
Epoch 18/20
14/14 - 0s - loss: 0.4921 - acc: 0.8096 - val_loss: 0.4927 - val_acc: 0.7986
Epoch 19/20
14/14 - 0s - loss: 0.5092 - acc: 0.8003 - val_loss: 0.4834 - val_acc: 0.8102
Epoch 20/20
14/14 - 0s - loss: 0.4941 - acc: 0.8079 - val_loss: 0.4774 - val_acc: 0.8218

Trial 2 Complete [00h 00m 14s]
val_acc: 0.8217592835426331

Best val_acc So Far: 0.8217592835426331
Total elapsed time: 00h 00m 38s

Search: Running Trial #3

Hyperparameter    |Value             |Best Value So Far 
filters           |128               |128               
max_pooling_num   |18                |14                
leraning_rate     |0.008             |0.002             
drop_out_rate     |0.4               |0.2               

2022-04-28 10:28:44.476159: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_17763"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:28:48.668470: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_19937"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0871 - acc: 0.3791 - val_loss: 1.0337 - val_acc: 0.4398
Epoch 2/20
14/14 - 0s - loss: 0.9657 - acc: 0.5122 - val_loss: 0.8302 - val_acc: 0.6528
Epoch 3/20
14/14 - 0s - loss: 0.8228 - acc: 0.6453 - val_loss: 0.7719 - val_acc: 0.7106
Epoch 4/20
14/14 - 0s - loss: 0.7220 - acc: 0.7106 - val_loss: 0.6743 - val_acc: 0.7454
Epoch 5/20
14/14 - 0s - loss: 0.6609 - acc: 0.7419 - val_loss: 0.5321 - val_acc: 0.7986
Epoch 6/20
14/14 - 0s - loss: 0.6228 - acc: 0.7575 - val_loss: 0.5674 - val_acc: 0.7708
Epoch 7/20
14/14 - 0s - loss: 0.5979 - acc: 0.7760 - val_loss: 0.5472 - val_acc: 0.7963
Epoch 8/20
14/14 - 0s - loss: 0.6233 - acc: 0.7581 - val_loss: 0.6492 - val_acc: 0.7060
Epoch 9/20
14/14 - 0s - loss: 0.6228 - acc: 0.7477 - val_loss: 0.5551 - val_acc: 0.7824
Epoch 10/20
14/14 - 0s - loss: 0.5688 - acc: 0.7824 - val_loss: 0.5374 - val_acc: 0.7847

Trial 3 Complete [00h 00m 10s]
val_acc: 0.7986111044883728

Best val_acc So Far: 0.8217592835426331
Total elapsed time: 00h 00m 48s

Search: Running Trial #4

Hyperparameter    |Value             |Best Value So Far 
filters           |512               |128               
max_pooling_num   |6                 |14                
leraning_rate     |0.001             |0.002             
drop_out_rate     |0.2               |0.2               

2022-04-28 10:28:54.670853: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_23399"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:28:59.125878: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_25573"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0426 - acc: 0.4734 - val_loss: 0.8816 - val_acc: 0.6481
Epoch 2/20
14/14 - 1s - loss: 0.8529 - acc: 0.6331 - val_loss: 0.7867 - val_acc: 0.6759
Epoch 3/20
14/14 - 1s - loss: 0.7239 - acc: 0.6991 - val_loss: 0.6852 - val_acc: 0.7083
Epoch 4/20
14/14 - 1s - loss: 0.6797 - acc: 0.7188 - val_loss: 0.6096 - val_acc: 0.7407
Epoch 5/20
14/14 - 1s - loss: 0.6297 - acc: 0.7471 - val_loss: 0.6553 - val_acc: 0.7176
Epoch 6/20
14/14 - 1s - loss: 0.6035 - acc: 0.7731 - val_loss: 0.5620 - val_acc: 0.7894
Epoch 7/20
14/14 - 1s - loss: 0.5570 - acc: 0.7830 - val_loss: 0.5522 - val_acc: 0.8009
Epoch 8/20
14/14 - 1s - loss: 0.5559 - acc: 0.7928 - val_loss: 0.5568 - val_acc: 0.7986
Epoch 9/20
14/14 - 1s - loss: 0.5257 - acc: 0.7986 - val_loss: 0.5522 - val_acc: 0.7963
Epoch 10/20
14/14 - 1s - loss: 0.5206 - acc: 0.7980 - val_loss: 0.5304 - val_acc: 0.8102
Epoch 11/20
14/14 - 1s - loss: 0.5343 - acc: 0.7922 - val_loss: 0.5408 - val_acc: 0.7847
Epoch 12/20
14/14 - 1s - loss: 0.5206 - acc: 0.7963 - val_loss: 0.5090 - val_acc: 0.8125
Epoch 13/20
14/14 - 1s - loss: 0.5184 - acc: 0.8021 - val_loss: 0.5258 - val_acc: 0.8079
Epoch 14/20
14/14 - 1s - loss: 0.5037 - acc: 0.8137 - val_loss: 0.4941 - val_acc: 0.8148
Epoch 15/20
14/14 - 1s - loss: 0.4993 - acc: 0.8212 - val_loss: 0.5147 - val_acc: 0.7963
Epoch 16/20
14/14 - 1s - loss: 0.4751 - acc: 0.8281 - val_loss: 0.4922 - val_acc: 0.8194
Epoch 17/20
14/14 - 1s - loss: 0.4700 - acc: 0.8235 - val_loss: 0.4989 - val_acc: 0.8056
Epoch 18/20
14/14 - 1s - loss: 0.4514 - acc: 0.8310 - val_loss: 0.4827 - val_acc: 0.8125
Epoch 19/20
14/14 - 1s - loss: 0.4720 - acc: 0.8218 - val_loss: 0.5453 - val_acc: 0.7917
Epoch 20/20
14/14 - 1s - loss: 0.4564 - acc: 0.8333 - val_loss: 0.5073 - val_acc: 0.7755

Trial 4 Complete [00h 00m 31s]
val_acc: 0.8194444179534912

Best val_acc So Far: 0.8217592835426331
Total elapsed time: 00h 01m 19s

Search: Running Trial #5

Hyperparameter    |Value             |Best Value So Far 
filters           |256               |128               
max_pooling_num   |12                |14                
leraning_rate     |0.0008            |0.002             
drop_out_rate     |0.4               |0.2               

2022-04-28 10:29:25.667582: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_31805"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:29:29.765037: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_33979"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0384 - acc: 0.4479 - val_loss: 0.9123 - val_acc: 0.6088
Epoch 2/20
14/14 - 1s - loss: 0.9364 - acc: 0.5608 - val_loss: 0.8074 - val_acc: 0.6458
Epoch 3/20
14/14 - 1s - loss: 0.8617 - acc: 0.6302 - val_loss: 0.7397 - val_acc: 0.6921
Epoch 4/20
14/14 - 1s - loss: 0.7820 - acc: 0.6661 - val_loss: 0.6816 - val_acc: 0.7477
Epoch 5/20
14/14 - 1s - loss: 0.7097 - acc: 0.7083 - val_loss: 0.6470 - val_acc: 0.7315
Epoch 6/20
14/14 - 1s - loss: 0.6682 - acc: 0.7251 - val_loss: 0.6056 - val_acc: 0.7523
Epoch 7/20
14/14 - 1s - loss: 0.6607 - acc: 0.7297 - val_loss: 0.6078 - val_acc: 0.7500
Epoch 8/20
14/14 - 1s - loss: 0.6310 - acc: 0.7494 - val_loss: 0.5893 - val_acc: 0.7662
Epoch 9/20
14/14 - 1s - loss: 0.6224 - acc: 0.7604 - val_loss: 0.5608 - val_acc: 0.7917
Epoch 10/20
14/14 - 1s - loss: 0.6009 - acc: 0.7656 - val_loss: 0.5984 - val_acc: 0.7593
Epoch 11/20
14/14 - 1s - loss: 0.6075 - acc: 0.7685 - val_loss: 0.5416 - val_acc: 0.8009
Epoch 12/20
14/14 - 1s - loss: 0.5888 - acc: 0.7668 - val_loss: 0.5460 - val_acc: 0.7778
Epoch 13/20
14/14 - 1s - loss: 0.5751 - acc: 0.7894 - val_loss: 0.5393 - val_acc: 0.7894
Epoch 14/20
14/14 - 1s - loss: 0.5652 - acc: 0.7824 - val_loss: 0.5516 - val_acc: 0.7870
Epoch 15/20
14/14 - 1s - loss: 0.5605 - acc: 0.7836 - val_loss: 0.5465 - val_acc: 0.7755
Epoch 16/20
14/14 - 1s - loss: 0.5560 - acc: 0.7859 - val_loss: 0.5265 - val_acc: 0.8009

Trial 5 Complete [00h 00m 15s]
val_acc: 0.8009259104728699

Best val_acc So Far: 0.8217592835426331
Total elapsed time: 00h 01m 34s

Search: Running Trial #6

Hyperparameter    |Value             |Best Value So Far 
filters           |32                |128               
max_pooling_num   |6                 |14                
leraning_rate     |0.001             |0.002             
drop_out_rate     |0.2               |0.2               

2022-04-28 10:29:41.032665: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_39103"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:29:45.144969: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_41277"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0125 - acc: 0.4774 - val_loss: 0.8777 - val_acc: 0.6296
Epoch 2/20
14/14 - 0s - loss: 0.8622 - acc: 0.6250 - val_loss: 0.7489 - val_acc: 0.6921
Epoch 3/20
14/14 - 0s - loss: 0.7526 - acc: 0.6829 - val_loss: 0.6781 - val_acc: 0.7153
Epoch 4/20
14/14 - 0s - loss: 0.6999 - acc: 0.7083 - val_loss: 0.7130 - val_acc: 0.7130
Epoch 5/20
14/14 - 0s - loss: 0.6751 - acc: 0.7216 - val_loss: 0.6394 - val_acc: 0.7569
Epoch 6/20
14/14 - 0s - loss: 0.6193 - acc: 0.7535 - val_loss: 0.5885 - val_acc: 0.7639
Epoch 7/20
14/14 - 0s - loss: 0.6018 - acc: 0.7645 - val_loss: 0.5860 - val_acc: 0.7546
Epoch 8/20
14/14 - 0s - loss: 0.6062 - acc: 0.7546 - val_loss: 0.5609 - val_acc: 0.7778
Epoch 9/20
14/14 - 0s - loss: 0.5743 - acc: 0.7807 - val_loss: 0.5727 - val_acc: 0.7685
Epoch 10/20
14/14 - 0s - loss: 0.5467 - acc: 0.7957 - val_loss: 0.5439 - val_acc: 0.7847
Epoch 11/20
14/14 - 0s - loss: 0.5481 - acc: 0.7865 - val_loss: 0.5594 - val_acc: 0.7778
Epoch 12/20
14/14 - 0s - loss: 0.5289 - acc: 0.7969 - val_loss: 0.5299 - val_acc: 0.7847
Epoch 13/20
14/14 - 0s - loss: 0.5259 - acc: 0.7934 - val_loss: 0.5620 - val_acc: 0.7870
Epoch 14/20
14/14 - 0s - loss: 0.5211 - acc: 0.7992 - val_loss: 0.5105 - val_acc: 0.8009
Epoch 15/20
14/14 - 0s - loss: 0.5145 - acc: 0.8021 - val_loss: 0.5058 - val_acc: 0.7894
Epoch 16/20
14/14 - 0s - loss: 0.5035 - acc: 0.8056 - val_loss: 0.4950 - val_acc: 0.8009
Epoch 17/20
14/14 - 0s - loss: 0.5128 - acc: 0.8056 - val_loss: 0.4963 - val_acc: 0.8032
Epoch 18/20
14/14 - 0s - loss: 0.4991 - acc: 0.8061 - val_loss: 0.5223 - val_acc: 0.8102
Epoch 19/20
14/14 - 0s - loss: 0.5024 - acc: 0.8131 - val_loss: 0.5195 - val_acc: 0.8079
Epoch 20/20
14/14 - 0s - loss: 0.4837 - acc: 0.8142 - val_loss: 0.4660 - val_acc: 0.8287

Trial 6 Complete [00h 00m 12s]
val_acc: 0.8287037014961243

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 01m 47s

Search: Running Trial #7

Hyperparameter    |Value             |Best Value So Far 
filters           |128               |32                
max_pooling_num   |12                |6                 
leraning_rate     |0.0004            |0.001             
drop_out_rate     |0.4               |0.2               

2022-04-28 10:29:53.573277: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_47817"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:29:57.731507: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_49991"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0869 - acc: 0.3825 - val_loss: 1.0416 - val_acc: 0.6088
Epoch 2/20
14/14 - 0s - loss: 1.0235 - acc: 0.5081 - val_loss: 0.9206 - val_acc: 0.5995
Epoch 3/20
14/14 - 0s - loss: 0.9638 - acc: 0.5434 - val_loss: 0.8866 - val_acc: 0.6343
Epoch 4/20
14/14 - 0s - loss: 0.9194 - acc: 0.5891 - val_loss: 0.8203 - val_acc: 0.6620
Epoch 5/20
14/14 - 0s - loss: 0.8735 - acc: 0.6186 - val_loss: 0.7876 - val_acc: 0.6505
Epoch 6/20
14/14 - 0s - loss: 0.8365 - acc: 0.6319 - val_loss: 0.7526 - val_acc: 0.6898
Epoch 7/20
14/14 - 0s - loss: 0.8071 - acc: 0.6528 - val_loss: 0.7083 - val_acc: 0.6944
Epoch 8/20
14/14 - 0s - loss: 0.7686 - acc: 0.6644 - val_loss: 0.6758 - val_acc: 0.7245
Epoch 9/20
14/14 - 0s - loss: 0.7462 - acc: 0.6979 - val_loss: 0.6553 - val_acc: 0.7338
Epoch 10/20
14/14 - 0s - loss: 0.7315 - acc: 0.6939 - val_loss: 0.6429 - val_acc: 0.7431
Epoch 11/20
14/14 - 0s - loss: 0.7244 - acc: 0.7072 - val_loss: 0.6249 - val_acc: 0.7384
Epoch 12/20
14/14 - 0s - loss: 0.6916 - acc: 0.7135 - val_loss: 0.6164 - val_acc: 0.7454
Epoch 13/20
14/14 - 0s - loss: 0.6769 - acc: 0.7216 - val_loss: 0.6006 - val_acc: 0.7662
Epoch 14/20
14/14 - 0s - loss: 0.6509 - acc: 0.7535 - val_loss: 0.5906 - val_acc: 0.7708
Epoch 15/20
14/14 - 0s - loss: 0.6546 - acc: 0.7413 - val_loss: 0.5799 - val_acc: 0.7755
Epoch 16/20
14/14 - 0s - loss: 0.6414 - acc: 0.7581 - val_loss: 0.5730 - val_acc: 0.7801
Epoch 17/20
14/14 - 0s - loss: 0.6309 - acc: 0.7650 - val_loss: 0.5716 - val_acc: 0.7685
Epoch 18/20
14/14 - 0s - loss: 0.6000 - acc: 0.7737 - val_loss: 0.5918 - val_acc: 0.7870
Epoch 19/20
14/14 - 0s - loss: 0.6190 - acc: 0.7622 - val_loss: 0.5567 - val_acc: 0.8032
Epoch 20/20
14/14 - 0s - loss: 0.6018 - acc: 0.7650 - val_loss: 0.5483 - val_acc: 0.7894

Trial 7 Complete [00h 00m 14s]
val_acc: 0.8032407164573669

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 02m 01s

Search: Running Trial #8

Hyperparameter    |Value             |Best Value So Far 
filters           |64                |32                
max_pooling_num   |8                 |6                 
leraning_rate     |0.002             |0.001             
drop_out_rate     |0.2               |0.2               

2022-04-28 10:30:07.761393: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_56993"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:30:11.986953: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_59167"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 0.9695 - acc: 0.5214 - val_loss: 0.8475 - val_acc: 0.6435
Epoch 2/20
14/14 - 0s - loss: 0.8179 - acc: 0.6435 - val_loss: 0.7048 - val_acc: 0.7176
Epoch 3/20
14/14 - 0s - loss: 0.7452 - acc: 0.6782 - val_loss: 0.7340 - val_acc: 0.7176
Epoch 4/20
14/14 - 0s - loss: 0.6864 - acc: 0.7020 - val_loss: 0.6376 - val_acc: 0.7593
Epoch 5/20
14/14 - 0s - loss: 0.6350 - acc: 0.7448 - val_loss: 0.5868 - val_acc: 0.7639
Epoch 6/20
14/14 - 0s - loss: 0.5928 - acc: 0.7610 - val_loss: 0.6604 - val_acc: 0.7431
Epoch 7/20
14/14 - 0s - loss: 0.6149 - acc: 0.7413 - val_loss: 0.5887 - val_acc: 0.7662
Epoch 8/20
14/14 - 0s - loss: 0.5667 - acc: 0.7812 - val_loss: 0.5392 - val_acc: 0.7940
Epoch 9/20
14/14 - 0s - loss: 0.5425 - acc: 0.7853 - val_loss: 0.6012 - val_acc: 0.7523
Epoch 10/20
14/14 - 0s - loss: 0.5406 - acc: 0.7836 - val_loss: 0.5281 - val_acc: 0.7963
Epoch 11/20
14/14 - 0s - loss: 0.5147 - acc: 0.7998 - val_loss: 0.5072 - val_acc: 0.8032
Epoch 12/20
14/14 - 0s - loss: 0.5639 - acc: 0.7882 - val_loss: 0.5400 - val_acc: 0.7847
Epoch 13/20
14/14 - 0s - loss: 0.5180 - acc: 0.8009 - val_loss: 0.6529 - val_acc: 0.7014
Epoch 14/20
14/14 - 0s - loss: 0.5537 - acc: 0.7784 - val_loss: 0.5037 - val_acc: 0.8171
Epoch 15/20
14/14 - 0s - loss: 0.5067 - acc: 0.8090 - val_loss: 0.5212 - val_acc: 0.7870
Epoch 16/20
14/14 - 0s - loss: 0.5150 - acc: 0.7951 - val_loss: 0.4953 - val_acc: 0.8125
Epoch 17/20
14/14 - 0s - loss: 0.4894 - acc: 0.8206 - val_loss: 0.4863 - val_acc: 0.8102
Epoch 18/20
14/14 - 0s - loss: 0.4980 - acc: 0.8154 - val_loss: 0.5464 - val_acc: 0.7523
Epoch 19/20
14/14 - 0s - loss: 0.5090 - acc: 0.8067 - val_loss: 0.5098 - val_acc: 0.7940

Trial 8 Complete [00h 00m 12s]
val_acc: 0.8171296119689941

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 02m 14s

Search: Running Trial #9

Hyperparameter    |Value             |Best Value So Far 
filters           |256               |32                
max_pooling_num   |18                |6                 
leraning_rate     |0.002             |0.001             
drop_out_rate     |0.5               |0.2               

2022-04-28 10:30:20.567638: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_65045"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:30:24.893267: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_67219"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0659 - acc: 0.3929 - val_loss: 0.9823 - val_acc: 0.4931
Epoch 2/20
14/14 - 1s - loss: 0.9550 - acc: 0.5365 - val_loss: 0.8586 - val_acc: 0.6759
Epoch 3/20
14/14 - 1s - loss: 0.8830 - acc: 0.6088 - val_loss: 0.8250 - val_acc: 0.6829
Epoch 4/20
14/14 - 1s - loss: 0.8287 - acc: 0.6447 - val_loss: 0.7034 - val_acc: 0.7222
Epoch 5/20
14/14 - 1s - loss: 0.7609 - acc: 0.6771 - val_loss: 0.6560 - val_acc: 0.7269
Epoch 6/20
14/14 - 1s - loss: 0.7127 - acc: 0.7164 - val_loss: 0.6130 - val_acc: 0.7685
Epoch 7/20
14/14 - 1s - loss: 0.6709 - acc: 0.7396 - val_loss: 0.6183 - val_acc: 0.7477
Epoch 8/20
14/14 - 1s - loss: 0.6583 - acc: 0.7436 - val_loss: 0.5927 - val_acc: 0.7778
Epoch 9/20
14/14 - 1s - loss: 0.6178 - acc: 0.7674 - val_loss: 0.5875 - val_acc: 0.7477
Epoch 10/20
14/14 - 1s - loss: 0.6161 - acc: 0.7604 - val_loss: 0.5565 - val_acc: 0.7870
Epoch 11/20
14/14 - 1s - loss: 0.6230 - acc: 0.7593 - val_loss: 0.5527 - val_acc: 0.7755
Epoch 12/20
14/14 - 1s - loss: 0.5999 - acc: 0.7581 - val_loss: 0.5380 - val_acc: 0.7870
Epoch 13/20
14/14 - 1s - loss: 0.5927 - acc: 0.7622 - val_loss: 0.5381 - val_acc: 0.8102
Epoch 14/20
14/14 - 1s - loss: 0.5842 - acc: 0.7743 - val_loss: 0.5726 - val_acc: 0.7708
Epoch 15/20
14/14 - 1s - loss: 0.5793 - acc: 0.7726 - val_loss: 0.5164 - val_acc: 0.8056
Epoch 16/20
14/14 - 1s - loss: 0.5758 - acc: 0.7807 - val_loss: 0.5176 - val_acc: 0.7963
Epoch 17/20
14/14 - 1s - loss: 0.5525 - acc: 0.7911 - val_loss: 0.5170 - val_acc: 0.8079
Epoch 18/20
14/14 - 1s - loss: 0.5632 - acc: 0.7841 - val_loss: 0.5138 - val_acc: 0.8056

Trial 9 Complete [00h 00m 16s]
val_acc: 0.8101851940155029

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 02m 30s

Search: Running Trial #10

Hyperparameter    |Value             |Best Value So Far 
filters           |256               |32                
max_pooling_num   |18                |6                 
leraning_rate     |0.008             |0.001             
drop_out_rate     |0.3               |0.2               

2022-04-28 10:30:36.998866: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_72897"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:30:41.203970: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_75071"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0872 - acc: 0.3872 - val_loss: 0.9045 - val_acc: 0.5741
Epoch 2/20
14/14 - 1s - loss: 0.9201 - acc: 0.5828 - val_loss: 0.7901 - val_acc: 0.6713
Epoch 3/20
14/14 - 1s - loss: 0.8561 - acc: 0.6279 - val_loss: 0.8390 - val_acc: 0.6227
Epoch 4/20
14/14 - 1s - loss: 0.8366 - acc: 0.6516 - val_loss: 0.7634 - val_acc: 0.6875
Epoch 5/20
14/14 - 1s - loss: 0.7959 - acc: 0.6586 - val_loss: 0.6934 - val_acc: 0.7130
Epoch 6/20
14/14 - 1s - loss: 0.7499 - acc: 0.6800 - val_loss: 0.6809 - val_acc: 0.7037
Epoch 7/20
14/14 - 1s - loss: 0.7179 - acc: 0.6910 - val_loss: 0.7246 - val_acc: 0.6875
Epoch 8/20
14/14 - 1s - loss: 0.7623 - acc: 0.6782 - val_loss: 0.6958 - val_acc: 0.6968
Epoch 9/20
14/14 - 1s - loss: 0.7041 - acc: 0.7118 - val_loss: 0.6576 - val_acc: 0.7315
Epoch 10/20
14/14 - 1s - loss: 0.6977 - acc: 0.7083 - val_loss: 0.6888 - val_acc: 0.7176
Epoch 11/20
14/14 - 1s - loss: 0.7310 - acc: 0.7124 - val_loss: 0.6185 - val_acc: 0.7500
Epoch 12/20
14/14 - 1s - loss: 0.6756 - acc: 0.7326 - val_loss: 0.5574 - val_acc: 0.7824
Epoch 13/20
14/14 - 1s - loss: 0.6469 - acc: 0.7552 - val_loss: 0.5684 - val_acc: 0.7662
Epoch 14/20
14/14 - 1s - loss: 0.6227 - acc: 0.7541 - val_loss: 0.5534 - val_acc: 0.7847
Epoch 15/20
14/14 - 1s - loss: 0.6125 - acc: 0.7622 - val_loss: 0.5812 - val_acc: 0.7639
Epoch 16/20
14/14 - 1s - loss: 0.6199 - acc: 0.7604 - val_loss: 0.5623 - val_acc: 0.7894
Epoch 17/20
14/14 - 1s - loss: 0.6100 - acc: 0.7529 - val_loss: 0.5432 - val_acc: 0.7986
Epoch 18/20
14/14 - 1s - loss: 0.6139 - acc: 0.7668 - val_loss: 0.5716 - val_acc: 0.7569
Epoch 19/20
14/14 - 1s - loss: 0.5859 - acc: 0.7789 - val_loss: 0.5784 - val_acc: 0.7708
Epoch 20/20
14/14 - 1s - loss: 0.6003 - acc: 0.7674 - val_loss: 0.5416 - val_acc: 0.7963

Trial 10 Complete [00h 00m 17s]
val_acc: 0.7986111044883728

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 02m 47s

Search: Running Trial #11

Hyperparameter    |Value             |Best Value So Far 
filters           |128               |32                
max_pooling_num   |10                |6                 
leraning_rate     |0.001             |0.001             
drop_out_rate     |0.4               |0.2               

2022-04-28 10:30:54.303240: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_81303"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:30:58.445322: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_83477"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0233 - acc: 0.4705 - val_loss: 0.8679 - val_acc: 0.6528
Epoch 2/20
14/14 - 0s - loss: 0.8960 - acc: 0.5914 - val_loss: 0.7975 - val_acc: 0.6574
Epoch 3/20
14/14 - 0s - loss: 0.8183 - acc: 0.6522 - val_loss: 0.6892 - val_acc: 0.7037
Epoch 4/20
14/14 - 0s - loss: 0.7317 - acc: 0.6973 - val_loss: 0.6574 - val_acc: 0.7315
Epoch 5/20
14/14 - 0s - loss: 0.6942 - acc: 0.7147 - val_loss: 0.6185 - val_acc: 0.7431
Epoch 6/20
14/14 - 0s - loss: 0.6461 - acc: 0.7402 - val_loss: 0.5984 - val_acc: 0.7639
Epoch 7/20
14/14 - 0s - loss: 0.6258 - acc: 0.7425 - val_loss: 0.6123 - val_acc: 0.7523
Epoch 8/20
14/14 - 0s - loss: 0.6204 - acc: 0.7558 - val_loss: 0.5762 - val_acc: 0.7894
Epoch 9/20
14/14 - 0s - loss: 0.5997 - acc: 0.7639 - val_loss: 0.5449 - val_acc: 0.7917
Epoch 10/20
14/14 - 0s - loss: 0.5691 - acc: 0.7772 - val_loss: 0.5487 - val_acc: 0.7894
Epoch 11/20
14/14 - 0s - loss: 0.5932 - acc: 0.7720 - val_loss: 0.5403 - val_acc: 0.7917
Epoch 12/20
14/14 - 0s - loss: 0.5641 - acc: 0.7899 - val_loss: 0.5226 - val_acc: 0.7940
Epoch 13/20
14/14 - 0s - loss: 0.5505 - acc: 0.7836 - val_loss: 0.5263 - val_acc: 0.8079
Epoch 14/20
14/14 - 0s - loss: 0.5491 - acc: 0.7940 - val_loss: 0.5310 - val_acc: 0.7963
Epoch 15/20
14/14 - 0s - loss: 0.5581 - acc: 0.7963 - val_loss: 0.5277 - val_acc: 0.7917
Epoch 16/20
14/14 - 0s - loss: 0.5584 - acc: 0.7812 - val_loss: 0.5119 - val_acc: 0.8056
Epoch 17/20
14/14 - 0s - loss: 0.5350 - acc: 0.8050 - val_loss: 0.5115 - val_acc: 0.8056
Epoch 18/20
14/14 - 0s - loss: 0.5295 - acc: 0.7963 - val_loss: 0.5246 - val_acc: 0.7940

Trial 11 Complete [00h 00m 13s]
val_acc: 0.8078703880310059

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 03m 01s

Search: Running Trial #12

Hyperparameter    |Value             |Best Value So Far 
filters           |256               |32                
max_pooling_num   |16                |6                 
leraning_rate     |0.0008            |0.001             
drop_out_rate     |0.1               |0.2               

2022-04-28 10:31:07.586317: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_89309"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:31:11.807237: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_91483"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0337 - acc: 0.4705 - val_loss: 0.8887 - val_acc: 0.6134
Epoch 2/20
14/14 - 1s - loss: 0.9066 - acc: 0.6059 - val_loss: 0.8257 - val_acc: 0.6597
Epoch 3/20
14/14 - 1s - loss: 0.8206 - acc: 0.6528 - val_loss: 0.7254 - val_acc: 0.6944
Epoch 4/20
14/14 - 1s - loss: 0.7424 - acc: 0.6748 - val_loss: 0.6863 - val_acc: 0.7037
Epoch 5/20
14/14 - 1s - loss: 0.6974 - acc: 0.7072 - val_loss: 0.6421 - val_acc: 0.7292
Epoch 6/20
14/14 - 1s - loss: 0.6564 - acc: 0.7292 - val_loss: 0.6554 - val_acc: 0.7106
Epoch 7/20
14/14 - 1s - loss: 0.6151 - acc: 0.7575 - val_loss: 0.5836 - val_acc: 0.7755
Epoch 8/20
14/14 - 1s - loss: 0.6007 - acc: 0.7610 - val_loss: 0.5866 - val_acc: 0.7569
Epoch 9/20
14/14 - 1s - loss: 0.5769 - acc: 0.7731 - val_loss: 0.5638 - val_acc: 0.7894
Epoch 10/20
14/14 - 1s - loss: 0.5860 - acc: 0.7674 - val_loss: 0.5763 - val_acc: 0.7639
Epoch 11/20
14/14 - 1s - loss: 0.5577 - acc: 0.7894 - val_loss: 0.5333 - val_acc: 0.7894
Epoch 12/20
14/14 - 1s - loss: 0.5211 - acc: 0.8009 - val_loss: 0.5669 - val_acc: 0.7824
Epoch 13/20
14/14 - 1s - loss: 0.5409 - acc: 0.7986 - val_loss: 0.5497 - val_acc: 0.7870
Epoch 14/20
14/14 - 1s - loss: 0.5400 - acc: 0.7899 - val_loss: 0.5249 - val_acc: 0.7940
Epoch 15/20
14/14 - 1s - loss: 0.5156 - acc: 0.8073 - val_loss: 0.5223 - val_acc: 0.8056
Epoch 16/20
14/14 - 1s - loss: 0.4890 - acc: 0.8206 - val_loss: 0.5176 - val_acc: 0.7963
Epoch 17/20
14/14 - 1s - loss: 0.5103 - acc: 0.8067 - val_loss: 0.5098 - val_acc: 0.8102
Epoch 18/20
14/14 - 1s - loss: 0.4967 - acc: 0.8079 - val_loss: 0.5009 - val_acc: 0.8102
Epoch 19/20
14/14 - 1s - loss: 0.4650 - acc: 0.8223 - val_loss: 0.5030 - val_acc: 0.8056
Epoch 20/20
14/14 - 1s - loss: 0.4756 - acc: 0.8223 - val_loss: 0.5176 - val_acc: 0.7963

Trial 12 Complete [00h 00m 17s]
val_acc: 0.8101851940155029

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 03m 18s

Search: Running Trial #13

Hyperparameter    |Value             |Best Value So Far 
filters           |256               |32                
max_pooling_num   |18                |6                 
leraning_rate     |0.0001            |0.001             
drop_out_rate     |0.3               |0.2               

2022-04-28 10:31:25.326770: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_97715"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:31:29.536174: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_99889"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0941 - acc: 0.3791 - val_loss: 1.0853 - val_acc: 0.3935
Epoch 2/20
14/14 - 1s - loss: 1.0791 - acc: 0.4358 - val_loss: 1.0632 - val_acc: 0.5903
Epoch 3/20
14/14 - 1s - loss: 1.0595 - acc: 0.4728 - val_loss: 1.0274 - val_acc: 0.6134
Epoch 4/20
14/14 - 1s - loss: 1.0317 - acc: 0.4902 - val_loss: 0.9853 - val_acc: 0.6204
Epoch 5/20
14/14 - 1s - loss: 1.0013 - acc: 0.5307 - val_loss: 0.9460 - val_acc: 0.6042
Epoch 6/20
14/14 - 1s - loss: 0.9783 - acc: 0.5341 - val_loss: 0.9101 - val_acc: 0.6204
Epoch 7/20
14/14 - 1s - loss: 0.9435 - acc: 0.5752 - val_loss: 0.8797 - val_acc: 0.6181
Epoch 8/20
14/14 - 1s - loss: 0.9304 - acc: 0.5729 - val_loss: 0.8508 - val_acc: 0.6366
Epoch 9/20
14/14 - 1s - loss: 0.9010 - acc: 0.6013 - val_loss: 0.8261 - val_acc: 0.6273
Epoch 10/20
14/14 - 1s - loss: 0.8783 - acc: 0.6157 - val_loss: 0.7952 - val_acc: 0.6644
Epoch 11/20
14/14 - 1s - loss: 0.8613 - acc: 0.6163 - val_loss: 0.7811 - val_acc: 0.6875
Epoch 12/20
14/14 - 1s - loss: 0.8384 - acc: 0.6441 - val_loss: 0.7466 - val_acc: 0.6829
Epoch 13/20
14/14 - 1s - loss: 0.8008 - acc: 0.6557 - val_loss: 0.7284 - val_acc: 0.7130
Epoch 14/20
14/14 - 1s - loss: 0.7879 - acc: 0.6534 - val_loss: 0.7008 - val_acc: 0.7176
Epoch 15/20
14/14 - 1s - loss: 0.7561 - acc: 0.6782 - val_loss: 0.6869 - val_acc: 0.7384
Epoch 16/20
14/14 - 1s - loss: 0.7587 - acc: 0.6806 - val_loss: 0.6823 - val_acc: 0.7176
Epoch 17/20
14/14 - 1s - loss: 0.7256 - acc: 0.6892 - val_loss: 0.6493 - val_acc: 0.7454
Epoch 18/20
14/14 - 1s - loss: 0.7038 - acc: 0.7153 - val_loss: 0.6418 - val_acc: 0.7454
Epoch 19/20
14/14 - 1s - loss: 0.7076 - acc: 0.7020 - val_loss: 0.6270 - val_acc: 0.7477
Epoch 20/20
14/14 - 1s - loss: 0.6952 - acc: 0.7280 - val_loss: 0.6316 - val_acc: 0.7569

Trial 13 Complete [00h 00m 17s]
val_acc: 0.7569444179534912

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 03m 36s

Search: Running Trial #14

Hyperparameter    |Value             |Best Value So Far 
filters           |32                |32                
max_pooling_num   |2                 |6                 
leraning_rate     |0.0008            |0.001             
drop_out_rate     |0.2               |0.2               

2022-04-28 10:31:42.716284: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_106583"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:31:46.803793: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_108757"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 0.9627 - acc: 0.5203 - val_loss: 0.8308 - val_acc: 0.6435
Epoch 2/20
14/14 - 0s - loss: 0.8314 - acc: 0.6262 - val_loss: 0.7147 - val_acc: 0.7153
Epoch 3/20
14/14 - 0s - loss: 0.7188 - acc: 0.7083 - val_loss: 0.6728 - val_acc: 0.7176
Epoch 4/20
14/14 - 0s - loss: 0.6612 - acc: 0.7240 - val_loss: 0.6440 - val_acc: 0.7361
Epoch 5/20
14/14 - 0s - loss: 0.6368 - acc: 0.7459 - val_loss: 0.5961 - val_acc: 0.7778
Epoch 6/20
14/14 - 0s - loss: 0.6264 - acc: 0.7488 - val_loss: 0.6077 - val_acc: 0.7454
Epoch 7/20
14/14 - 0s - loss: 0.6080 - acc: 0.7679 - val_loss: 0.5636 - val_acc: 0.7870
Epoch 8/20
14/14 - 0s - loss: 0.5482 - acc: 0.7922 - val_loss: 0.5882 - val_acc: 0.7824
Epoch 9/20
14/14 - 0s - loss: 0.5559 - acc: 0.7772 - val_loss: 0.5343 - val_acc: 0.8032
Epoch 10/20
14/14 - 0s - loss: 0.5204 - acc: 0.8032 - val_loss: 0.5437 - val_acc: 0.7870
Epoch 11/20
14/14 - 0s - loss: 0.5119 - acc: 0.8038 - val_loss: 0.5095 - val_acc: 0.8125
Epoch 12/20
14/14 - 0s - loss: 0.5149 - acc: 0.8009 - val_loss: 0.5493 - val_acc: 0.7731
Epoch 13/20
14/14 - 0s - loss: 0.5275 - acc: 0.7969 - val_loss: 0.5212 - val_acc: 0.7870
Epoch 14/20
14/14 - 0s - loss: 0.4856 - acc: 0.8252 - val_loss: 0.5193 - val_acc: 0.8009
Epoch 15/20
14/14 - 0s - loss: 0.4908 - acc: 0.8171 - val_loss: 0.5021 - val_acc: 0.8241
Epoch 16/20
14/14 - 0s - loss: 0.4706 - acc: 0.8218 - val_loss: 0.4873 - val_acc: 0.8079
Epoch 17/20
14/14 - 0s - loss: 0.4634 - acc: 0.8287 - val_loss: 0.4984 - val_acc: 0.8056
Epoch 18/20
14/14 - 0s - loss: 0.4570 - acc: 0.8264 - val_loss: 0.4870 - val_acc: 0.8148
Epoch 19/20
14/14 - 0s - loss: 0.4629 - acc: 0.8258 - val_loss: 0.4649 - val_acc: 0.8194
Epoch 20/20
14/14 - 0s - loss: 0.4593 - acc: 0.8258 - val_loss: 0.5322 - val_acc: 0.8009

Trial 14 Complete [00h 00m 13s]
val_acc: 0.8240740895271301

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 03m 49s

Search: Running Trial #15

Hyperparameter    |Value             |Best Value So Far 
filters           |64                |32                
max_pooling_num   |12                |6                 
leraning_rate     |0.0001            |0.001             
drop_out_rate     |0.2               |0.2               

2022-04-28 10:31:56.381608: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_114835"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:32:00.548264: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_117009"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 6s - loss: 1.0944 - acc: 0.3646 - val_loss: 1.0840 - val_acc: 0.4097
Epoch 2/20
14/14 - 0s - loss: 1.0818 - acc: 0.4676 - val_loss: 1.0672 - val_acc: 0.6250
Epoch 3/20
14/14 - 0s - loss: 1.0664 - acc: 0.4792 - val_loss: 1.0443 - val_acc: 0.5880
Epoch 4/20
14/14 - 0s - loss: 1.0422 - acc: 0.5301 - val_loss: 1.0096 - val_acc: 0.6181
Epoch 5/20
14/14 - 0s - loss: 1.0098 - acc: 0.5637 - val_loss: 0.9640 - val_acc: 0.6134
Epoch 6/20
14/14 - 0s - loss: 0.9701 - acc: 0.5862 - val_loss: 0.9186 - val_acc: 0.6250
Epoch 7/20
14/14 - 0s - loss: 0.9419 - acc: 0.5868 - val_loss: 0.8834 - val_acc: 0.6181

Trial 15 Complete [00h 00m 08s]
val_acc: 0.625

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 03m 58s

Search: Running Trial #16

Hyperparameter    |Value             |Best Value So Far 
filters           |256               |32                
max_pooling_num   |16                |6                 
leraning_rate     |0.0004            |0.001             
drop_out_rate     |0.3               |0.2               

2022-04-28 10:32:04.684221: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_119409"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:32:08.967189: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_121583"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 5s - loss: 1.0723 - acc: 0.4126 - val_loss: 1.0096 - val_acc: 0.5255
Epoch 2/20
14/14 - 1s - loss: 0.9992 - acc: 0.4954 - val_loss: 0.9005 - val_acc: 0.5995
Epoch 3/20
14/14 - 1s - loss: 0.9438 - acc: 0.5793 - val_loss: 0.8550 - val_acc: 0.6435
Epoch 4/20
14/14 - 1s - loss: 0.8883 - acc: 0.5984 - val_loss: 0.7810 - val_acc: 0.6597
Epoch 5/20
14/14 - 1s - loss: 0.8379 - acc: 0.6302 - val_loss: 0.7363 - val_acc: 0.6852
Epoch 6/20
14/14 - 1s - loss: 0.7880 - acc: 0.6632 - val_loss: 0.7257 - val_acc: 0.6968
Epoch 7/20
14/14 - 1s - loss: 0.7822 - acc: 0.6591 - val_loss: 0.7071 - val_acc: 0.6782
Epoch 8/20
14/14 - 1s - loss: 0.7392 - acc: 0.6846 - val_loss: 0.6522 - val_acc: 0.7454
Epoch 9/20
14/14 - 1s - loss: 0.7075 - acc: 0.7037 - val_loss: 0.6611 - val_acc: 0.7431
Epoch 10/20
14/14 - 1s - loss: 0.6743 - acc: 0.7188 - val_loss: 0.6154 - val_acc: 0.7778
Epoch 11/20
14/14 - 1s - loss: 0.6729 - acc: 0.7211 - val_loss: 0.5864 - val_acc: 0.7708
Epoch 12/20
14/14 - 1s - loss: 0.6425 - acc: 0.7477 - val_loss: 0.5723 - val_acc: 0.7894
Epoch 13/20
14/14 - 1s - loss: 0.6033 - acc: 0.7656 - val_loss: 0.5648 - val_acc: 0.7917
Epoch 14/20
14/14 - 1s - loss: 0.5999 - acc: 0.7656 - val_loss: 0.5534 - val_acc: 0.7940
Epoch 15/20
14/14 - 1s - loss: 0.5972 - acc: 0.7564 - val_loss: 0.5487 - val_acc: 0.7963
Epoch 16/20
14/14 - 1s - loss: 0.5841 - acc: 0.7685 - val_loss: 0.5501 - val_acc: 0.8148
Epoch 17/20
14/14 - 1s - loss: 0.5988 - acc: 0.7726 - val_loss: 0.5387 - val_acc: 0.7940
Epoch 18/20
14/14 - 1s - loss: 0.5631 - acc: 0.7801 - val_loss: 0.5258 - val_acc: 0.7986
Epoch 19/20
14/14 - 1s - loss: 0.5543 - acc: 0.7841 - val_loss: 0.5278 - val_acc: 0.8009
Epoch 20/20
14/14 - 1s - loss: 0.5379 - acc: 0.8044 - val_loss: 0.5400 - val_acc: 0.8009

Trial 16 Complete [00h 00m 16s]
val_acc: 0.8148148059844971

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 04m 15s

Search: Running Trial #17

Hyperparameter    |Value             |Best Value So Far 
filters           |512               |32                
max_pooling_num   |4                 |6                 
leraning_rate     |0.008             |0.001             
drop_out_rate     |0.2               |0.2               

2022-04-28 10:32:21.671165: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_128277"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Epoch 1/20
2022-04-28 10:32:27.433950: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_130451"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
14/14 - 7s - loss: 4.4782 - acc: 0.3848 - val_loss: 0.9408 - val_acc: 0.5833
Epoch 2/20
14/14 - 1s - loss: 0.9320 - acc: 0.5712 - val_loss: 0.8440 - val_acc: 0.6551
Epoch 3/20
14/14 - 1s - loss: 0.8646 - acc: 0.6146 - val_loss: 0.8482 - val_acc: 0.6389
Epoch 4/20
14/14 - 1s - loss: 0.8180 - acc: 0.6615 - val_loss: 0.7367 - val_acc: 0.7245
Epoch 5/20
14/14 - 1s - loss: 0.7236 - acc: 0.6910 - val_loss: 0.6752 - val_acc: 0.7014
Epoch 6/20
14/14 - 1s - loss: 0.7341 - acc: 0.6933 - val_loss: 0.6784 - val_acc: 0.7176
Epoch 7/20
14/14 - 1s - loss: 0.6836 - acc: 0.7193 - val_loss: 0.6455 - val_acc: 0.7384
Epoch 8/20
14/14 - 1s - loss: 0.6353 - acc: 0.7436 - val_loss: 0.6111 - val_acc: 0.7523
Epoch 9/20
14/14 - 1s - loss: 0.5930 - acc: 0.7604 - val_loss: 0.5840 - val_acc: 0.7801
Epoch 10/20
14/14 - 1s - loss: 0.5612 - acc: 0.7824 - val_loss: 0.5691 - val_acc: 0.7662
Epoch 11/20
14/14 - 1s - loss: 0.5813 - acc: 0.7737 - val_loss: 0.5853 - val_acc: 0.7662
Epoch 12/20
14/14 - 1s - loss: 0.5558 - acc: 0.7870 - val_loss: 0.5326 - val_acc: 0.7940
Epoch 13/20
14/14 - 1s - loss: 0.5193 - acc: 0.7969 - val_loss: 0.6798 - val_acc: 0.6991
Epoch 14/20
14/14 - 1s - loss: 0.5596 - acc: 0.7824 - val_loss: 0.7020 - val_acc: 0.6829
Epoch 15/20
14/14 - 1s - loss: 0.5890 - acc: 0.7604 - val_loss: 0.5429 - val_acc: 0.7917
Epoch 16/20
14/14 - 1s - loss: 0.5219 - acc: 0.8003 - val_loss: 0.5069 - val_acc: 0.8125
Epoch 17/20
14/14 - 1s - loss: 0.5012 - acc: 0.8166 - val_loss: 0.5195 - val_acc: 0.7801
Epoch 18/20
14/14 - 1s - loss: 0.4999 - acc: 0.8061 - val_loss: 0.5579 - val_acc: 0.7847
Epoch 19/20
14/14 - 1s - loss: 0.4729 - acc: 0.8218 - val_loss: 0.4893 - val_acc: 0.8241
Epoch 20/20
14/14 - 1s - loss: 0.4668 - acc: 0.8160 - val_loss: 0.4971 - val_acc: 0.8125

Trial 17 Complete [00h 00m 39s]
val_acc: 0.8240740895271301

Best val_acc So Far: 0.8287037014961243
Total elapsed time: 00h 04m 54s
Results summary
Results in random_search_results/LA-RandomSearchHyperparametersOptimization
Showing 10 best trials
Objective(name='val_acc', direction='max')
Trial summary
Hyperparameters:
filters: 32
max_pooling_num: 6
leraning_rate: 0.001
drop_out_rate: 0.2
Score: 0.8287037014961243
Trial summary
Hyperparameters:
filters: 32
max_pooling_num: 2
leraning_rate: 0.0008
drop_out_rate: 0.2
Score: 0.8240740895271301
Trial summary
Hyperparameters:
filters: 512
max_pooling_num: 4
leraning_rate: 0.008
drop_out_rate: 0.2
Score: 0.8240740895271301
Trial summary
Hyperparameters:
filters: 128
max_pooling_num: 14
leraning_rate: 0.002
drop_out_rate: 0.2
Score: 0.8217592835426331
Trial summary
Hyperparameters:
filters: 256
max_pooling_num: 12
leraning_rate: 0.01
drop_out_rate: 0.2
Score: 0.8194444179534912
Trial summary
Hyperparameters:
filters: 512
max_pooling_num: 6
leraning_rate: 0.001
drop_out_rate: 0.2
Score: 0.8194444179534912
Trial summary
Hyperparameters:
filters: 64
max_pooling_num: 8
leraning_rate: 0.002
drop_out_rate: 0.2
Score: 0.8171296119689941
Trial summary
Hyperparameters:
filters: 256
max_pooling_num: 16
leraning_rate: 0.0004
drop_out_rate: 0.30000000000000004
Score: 0.8148148059844971
Trial summary
Hyperparameters:
filters: 256
max_pooling_num: 18
leraning_rate: 0.002
drop_out_rate: 0.5
Score: 0.8101851940155029
Trial summary
Hyperparameters:
filters: 256
max_pooling_num: 16
leraning_rate: 0.0008
drop_out_rate: 0.1
Score: 0.8101851940155029
None
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d (Conv1D)              (None, 3000, 32)          352       
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 500, 32)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 500, 32)           10272     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 83, 32)            0         
_________________________________________________________________
dropout (Dropout)            (None, 83, 32)            0         
_________________________________________________________________
flatten (Flatten)            (None, 2656)              0         
_________________________________________________________________
dense (Dense)                (None, 128)               340096    
_________________________________________________________________
dense_1 (Dense)              (None, 64)                8256      
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 195       
=================================================================
Total params: 359,171
Trainable params: 359,171
Non-trainable params: 0
_________________________________________________________________
Best model structure: None
2022-04-28 10:33:00.704989: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_136542"
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
17/17 - 3s - loss: 0.4585 - acc: 0.8148
Best model test results: 0.8148148059844971
